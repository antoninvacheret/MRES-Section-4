{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10172542-bf25-4efc-bf2e-dad7b1d0337d",
   "metadata": {},
   "source": [
    "# Section 4 - Computer vision-based machine learning \n",
    "## Introduction to pyTorch models \n",
    "\n",
    "## Dr. Antonin Vacheret (avachere@imperial.ac.uk) \n",
    "## High Energy Physics Group\n",
    "## 523 Blackett Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739023fe-e01f-441c-8453-e7321168edee",
   "metadata": {},
   "source": [
    "A quick run through some basics of pyTorch starting from a quick exploration of the models readily available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b573018-6238-4fac-8a73-762f825dcba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "torch.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d6020b-3b42-472d-ab94-c3fe000b85d4",
   "metadata": {},
   "source": [
    "## I. Pre-trained Legacy computer vision classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90cbece-348b-4b08-b44d-cc2016876c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "dir(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e122f1ef-4db2-4f65-be38-0eb16ebb86d5",
   "metadata": {},
   "source": [
    "This is the famous AlexNet model that shaked the field of machine learning in 2012:\n",
    "https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
    "\n",
    "Note: the lowercase models have fixed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee1823-fb81-4623-9cdd-80bfa76bcf34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alexnet_function = models.AlexNet() # this is the \"empty shell\" of Alexnet\n",
    "alexnet_trained = models.alexnet(pretrained=True) # fixed artchitecture already pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4bd96d-9156-4012-af29-8765cf16cbec",
   "metadata": {},
   "source": [
    "This one is Resnet 101 which stands for residual network. This one is the 101 layer version.\n",
    "https://arxiv.org/abs/1512.03385\n",
    "It has beaten several benchmark in 2015 and started the deep learning revolution. It is trained on imagenet with 1.2M images on 1000 categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7438cb0d-d6b3-436f-9569-4f5f46cf17e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet101(pretrained=True) # beware this is taking on average a few mins to download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf14bfb-666a-4f53-9c4d-0a5f27681e83",
   "metadata": {},
   "source": [
    "Let's take a look at a high def picture of a dog. You can replace this one with your prefered one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf89bc5-0895-46c4-adfe-01e6f98d0e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(\"img/mydoge.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98abc99c-11d4-4314-86a6-11a5f586441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4988eb-a248-4688-99f0-561da20db560",
   "metadata": {},
   "source": [
    "Importing Hi-definition image from img folder but now defining some transformation first (a very powerful feature of pytorch !) to preprocess the image and get the right input size for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de002aa6-dae6-48b4-9662-46030784167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2f5fd-e8f7-4ca4-a09a-2ef13da1e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = preprocess(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab83669-2a26-44c7-a0ef-ebaf6f8bfcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc7cb35-bd52-49b3-819f-6113a1575cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_t[2,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c2c21-f439-4438-be7b-54824209946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.unsqueeze(img_t, 0)\n",
    "batch_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2545531-aac5-4076-8246-0310e5b1beb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resnet.eval() # putting the model in inference mode (no training of the weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93452d2e-9081-405c-b800-e03c76f3c764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = resnet(batch_t)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276649d-9485-4a8e-8e73-e54c5359ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores  = out.detach().numpy()\n",
    "#plt.plot(scores[0])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab0dd58-b237-4d80-9c61-eeba3fd594c5",
   "metadata": {},
   "source": [
    "#### Now an operation involving a massive 44.5M parameters has just taken place !\n",
    "This has produced a vector of a 1000 score, one for each label of the imagenet training set. Let's get the file that has the imagenet list of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737885f-8b65-4db3-8d10-6461c957cd76",
   "metadata": {},
   "source": [
    "We need now to figure out what was the ranking for our dog picture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec7f28-6893-4520-a8e9-46f28d10283f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8e8e5-4409-4386-8010-66cbd20be7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, index = torch.max(out, 1) # this returns the value and index of the higest score\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1f9c2b-63e6-4821-a530-c58fd2a166f7",
   "metadata": {},
   "source": [
    "Resnet gives us a score but what we are interested in is more something like a the probability of being of a certain category. We will use the softmax function for that (multi-class classifier). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487aeef-9ad7-4511-b4b2-c668e3ec3e11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "percentage = torch.nn.functional.softmax(out, dim=1)[0] # only one dimension, [0] is to return one value.\n",
    "percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26168c52-8190-46ef-be3a-5f8d6ea0ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[index[0]], percentage[index[0]].item() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee1ee4-906d-4263-b950-629dba65c5a3",
   "metadata": {},
   "source": [
    "Exercises:\n",
    "\n",
    "* Sort the output so the five highest probabilities come out from the resnet outpout\n",
    "    \n",
    "* Dowload alexnet and look at the output for our dog image. Which model is best ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e1fc77-1d5a-4239-9d87-69833ccee7a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## II. Pre-trained example of another type of model: the CycleGAN \n",
    "from Deep Learning with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb09a5-3bc8-42f3-a2d0-14131c62beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResNetBlock(nn.Module): # <1>\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim)\n",
    "\n",
    "    def build_conv_block(self, dim):\n",
    "        conv_block = []\n",
    "\n",
    "        conv_block += [nn.ReflectionPad2d(1)]\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n",
    "                       nn.InstanceNorm2d(dim),\n",
    "                       nn.ReLU(True)]\n",
    "\n",
    "        conv_block += [nn.ReflectionPad2d(1)]\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n",
    "                       nn.InstanceNorm2d(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x) # <2>\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_nc=3, output_nc=3, ngf=64, n_blocks=9): # <3> \n",
    "\n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResNetGenerator, self).__init__()\n",
    "\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.ngf = ngf\n",
    "\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=True),\n",
    "                 nn.InstanceNorm2d(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n",
    "                                stride=2, padding=1, bias=True),\n",
    "                      nn.InstanceNorm2d(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        mult = 2**n_downsampling\n",
    "        for i in range(n_blocks):\n",
    "            model += [ResNetBlock(ngf * mult)]\n",
    "\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**(n_downsampling - i)\n",
    "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                         kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1,\n",
    "                                         bias=True),\n",
    "                      nn.InstanceNorm2d(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        model += [nn.ReflectionPad2d(3)]\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "        model += [nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input): # <3>\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3748a39-b85d-4e56-8dc3-8dd241b3c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = ResNetGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60549b5-bc5a-4203-a686-6f41642cd5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'data/horse2zebra_0.4.0.pth'\n",
    "model_data = torch.load(model_path)\n",
    "netG.load_state_dict(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c0488-6a99-4852-9bd7-41bd662437b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "netG.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314402ab-ab56-481f-980a-2c10eadc94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9df38c-0d3e-41d7-99da-0dce8757e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([transforms.Resize(256),\n",
    "                                 transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278970e0-404e-4047-90c5-ac0af6e7dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"img/horse.jpg\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870abe47-939b-4795-88ba-7f96395c662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = preprocess(img)\n",
    "batch_t = torch.unsqueeze(img_t, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b54112-e291-4da5-9a52-68cc9a8c0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_out = netG(batch_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4296ef8f-276a-4bd5-a931-b22e74efbe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_t = (batch_out.data.squeeze() + 1.0) / 2.0\n",
    "out_img = transforms.ToPILImage()(out_t)\n",
    "# out_img.save('data/zebra.jpg')\n",
    "out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c31fe0-c920-49bd-b414-103d0ce6fb46",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
